{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic libraries needed\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "import keras.api._v2.keras as keras\n",
    "from keras import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, TimeDistributed, Conv1D, MaxPooling1D, Flatten, Bidirectional, Input, Flatten, Activation, Reshape, RepeatVector, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from numpy import array\n",
    "\n",
    "data_hor=pd.read_csv(\"Data/combined_data_ver.csv\")\n",
    "data_hor = data_hor.drop('Unnamed: 0', axis=1)\n",
    "data_hor.head()\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "power_list = [i for i in data_hor.columns]\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "perc=0.7\n",
    "\n",
    "train_size = int(len(data_hor) * perc)\n",
    "test_size = len(data_hor) - train_size\n",
    "df_train = data_hor[0:train_size]\n",
    "df_test = data_hor[train_size:len(data_hor)]\n",
    "\n",
    "scaled_train = scaler.fit_transform(df_train[power_list])\n",
    "scaled_test = scaler.transform(df_test[power_list])\n",
    "\n",
    "def split_sequence(sequence, look_back, forecast_horizon):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)): \n",
    "        lag_end = i + look_back\n",
    "        forecast_end = lag_end + forecast_horizon\n",
    "        if forecast_end > len(sequence):\n",
    "            break\n",
    "        seq_x, seq_y = sequence[i:lag_end], sequence[lag_end:forecast_end]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "        \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def evaluate_forecast(y_test_inverse, yhat_inverse):\n",
    "    mse_ = tf.keras.losses.MeanSquaredError()\n",
    "    mae_ = tf.keras.losses.MeanAbsoluteError()\n",
    "    mape_ = tf.keras.losses.MeanAbsolutePercentageError()\n",
    "    mae = mae_(y_test_inverse,yhat_inverse)\n",
    "    print('mae:', mae)\n",
    "    mse = mse_(y_test_inverse,yhat_inverse)\n",
    "    print('mse:', mse)\n",
    "    mape = mape_(y_test_inverse,yhat_inverse)\n",
    "    print('mape:', mape)\n",
    "\n",
    "LOOK_BACK=500\n",
    "FORECAST_RANGE=20\n",
    "\n",
    "## making the look_back according to the main frequency since the data are kindly periodic\n",
    "\n",
    "n_features=len(power_list)\n",
    "\n",
    "X_train, y_train = split_sequence(scaled_train, look_back=LOOK_BACK, forecast_horizon=FORECAST_RANGE)\n",
    "X_test, y_test = split_sequence(scaled_test, look_back=LOOK_BACK, forecast_horizon=FORECAST_RANGE)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "def inverse_transform(y_test, yhat):\n",
    "    y_test_reshaped = y_test.reshape(-1, y_test.shape[-1])\n",
    "    yhat_reshaped = yhat.reshape(-1, yhat.shape[-1])\n",
    "    yhat_inverse = scaler.inverse_transform(yhat_reshaped)\n",
    "    y_test_inverse = scaler.inverse_transform(y_test_reshaped)\n",
    "    return yhat_inverse, y_test_inverse\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "validation = 0.1\n",
    "\n",
    "checkpoint_filepath = 'improvementsModel_combinedData_ver_20_mac_CNN_LSTM/weights-improvement-{epoch:06d}-{val_loss:.6f}.hdf5'\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.005,\n",
    "    patience=10,\n",
    "    mode='min'\n",
    ")\n",
    "rlrop_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.2, mode='min', patience=3, min_lr=0.005)\n",
    "\n",
    "## CNN-LSTM Encoder - Decoder Model\n",
    "\n",
    "model_enc_dec_cnn = Sequential()\n",
    "model_enc_dec_cnn.add(Conv1D(filters=64, kernel_size=9, activation='tanh', input_shape=(LOOK_BACK, n_features)))\n",
    "model_enc_dec_cnn.add(Conv1D(filters=64, kernel_size=11, activation='tanh'))\n",
    "model_enc_dec_cnn.add(MaxPooling1D(pool_size=2))\n",
    "model_enc_dec_cnn.add(Flatten())\n",
    "model_enc_dec_cnn.add(RepeatVector(FORECAST_RANGE))\n",
    "model_enc_dec_cnn.add(LSTM(200, activation='tanh', return_sequences=True))\n",
    "model_enc_dec_cnn.add(TimeDistributed(Dense(100, activation='tanh')))\n",
    "model_enc_dec_cnn.add(TimeDistributed(Dense(n_features)))\n",
    "model_enc_dec_cnn.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "plot_model(model=model_enc_dec_cnn, show_shapes=True)\n",
    "history = model_enc_dec_cnn.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation,callbacks=[early_stopping_callback, checkpoint_callback, rlrop_callback])\n",
    "yhat = model_enc_dec_cnn.predict(X_test, verbose=0)\n",
    "yhat_inverse, y_test_inverse = inverse_transform(y_test, yhat)\n",
    "evaluate_forecast(y_test_inverse, yhat_inverse)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
